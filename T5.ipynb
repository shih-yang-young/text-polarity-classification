{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a2fd8c-7f40-4030-9f90-57c99236195f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>director dirk shafer and co-writer greg hinton...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a charming , quirky and leisurely paced scotti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the price was good ,  and came quickly though ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i was looking forward to this game for a coupl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>arguably the year 's silliest and most incoher...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>an imaginative comedy\\/thriller .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>a savvy exploration of paranoia and insecurity...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>on the other hand for power grating you ve got...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>like dickens with his passages , mcgrath craft...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>those who would follow haneke on his creepy ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id                                               TEXT  LABEL\n",
       "0          0  director dirk shafer and co-writer greg hinton...      0\n",
       "1          1  a charming , quirky and leisurely paced scotti...      1\n",
       "2          2  the price was good ,  and came quickly though ...      1\n",
       "3          3  i was looking forward to this game for a coupl...      0\n",
       "4          4  arguably the year 's silliest and most incoher...      0\n",
       "...      ...                                                ...    ...\n",
       "1995    1995                  an imaginative comedy\\/thriller .      1\n",
       "1996    1996  a savvy exploration of paranoia and insecurity...      1\n",
       "1997    1997  on the other hand for power grating you ve got...      1\n",
       "1998    1998  like dickens with his passages , mcgrath craft...      1\n",
       "1999    1999  those who would follow haneke on his creepy ex...      1\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data = pd.read_csv('train_2022.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f99116d-b2f5-4a6c-97f3-22621d434dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good to know if you can t find these elsewhere .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>love it !  the grill plates come out and pop i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i m convinced this was a poorly executed refur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i would never have complained about that if it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the photo shows the same whole ,  large candie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>10995</td>\n",
       "      <td>i didn t quite get it the first time .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>10996</td>\n",
       "      <td>i ve tried installing with and without the oem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>10997</td>\n",
       "      <td>i was parked at a truck stop in the cincinnati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>10998</td>\n",
       "      <td>i recently bought this case after seeing some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>10999</td>\n",
       "      <td>the keyboard types only % of the time and the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_id                                               TEXT\n",
       "0           0   good to know if you can t find these elsewhere .\n",
       "1           1  love it !  the grill plates come out and pop i...\n",
       "2           2  i m convinced this was a poorly executed refur...\n",
       "3           3  i would never have complained about that if it...\n",
       "4           4  the photo shows the same whole ,  large candie...\n",
       "...       ...                                                ...\n",
       "10995   10995             i didn t quite get it the first time .\n",
       "10996   10996  i ve tried installing with and without the oem...\n",
       "10997   10997  i was parked at a truck stop in the cincinnati...\n",
       "10998   10998  i recently bought this case after seeing some ...\n",
       "10999   10999  the keyboard types only % of the time and the ...\n",
       "\n",
       "[11000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460aabd6-8250-4f59-836b-a9e8537e3c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72920e-bc66-4ff1-b46a-6a1cddede43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e502b402-a20e-4d19-bb60-673ae433130d",
   "metadata": {},
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33fed1-d253-406b-93b1-db502cbd9fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0679283142089844\n",
      "Epoch 0, Loss: 2.128352403640747\n",
      "Epoch 0, Loss: 2.1275112628936768\n",
      "Epoch 0, Loss: 1.7556647062301636\n",
      "Epoch 0, Loss: 1.7121444940567017\n",
      "Epoch 0, Loss: 1.5263192653656006\n",
      "Epoch 0, Loss: 1.6478596925735474\n",
      "Epoch 0, Loss: 1.6046369075775146\n",
      "Epoch 0, Loss: 1.4518781900405884\n",
      "Epoch 0, Loss: 1.420722246170044\n",
      "Epoch 0, Loss: 1.2442291975021362\n",
      "Epoch 0, Loss: 1.2705384492874146\n",
      "Epoch 0, Loss: 0.9988387227058411\n",
      "Epoch 0, Loss: 1.0272718667984009\n",
      "Epoch 0, Loss: 0.9811387658119202\n",
      "Epoch 0, Loss: 0.9191348552703857\n",
      "Epoch 0, Loss: 0.7155396342277527\n",
      "Epoch 0, Loss: 0.7217323780059814\n",
      "Epoch 0, Loss: 0.6201804876327515\n",
      "Epoch 0, Loss: 0.5397351980209351\n",
      "Epoch 0, Loss: 0.432305246591568\n",
      "Epoch 0, Loss: 0.3552534282207489\n",
      "Epoch 0, Loss: 0.32101693749427795\n",
      "Epoch 0, Loss: 0.2773313820362091\n",
      "Epoch 0, Loss: 0.2618369460105896\n",
      "Epoch 0, Loss: 0.21358676254749298\n",
      "Epoch 0, Loss: 0.18035222589969635\n",
      "Epoch 0, Loss: 0.1817421019077301\n",
      "Epoch 0, Loss: 0.1652391254901886\n",
      "Epoch 0, Loss: 0.15602125227451324\n",
      "Epoch 0, Loss: 0.14150461554527283\n",
      "Epoch 0, Loss: 0.14285926520824432\n",
      "Epoch 0, Loss: 0.12005360424518585\n",
      "Epoch 0, Loss: 0.12898485362529755\n",
      "Epoch 0, Loss: 0.12069452553987503\n",
      "Epoch 0, Loss: 0.11609593778848648\n",
      "Epoch 0, Loss: 0.10436266660690308\n",
      "Epoch 0, Loss: 0.10546843707561493\n",
      "Epoch 0, Loss: 0.1147942766547203\n",
      "Epoch 0, Loss: 0.09495310485363007\n",
      "Epoch 0, Loss: 0.10092449188232422\n",
      "Epoch 0, Loss: 0.10394211113452911\n",
      "Epoch 0, Loss: 0.09532592445611954\n",
      "Epoch 0, Loss: 0.10299567133188248\n",
      "Epoch 0, Loss: 0.08938390016555786\n",
      "Epoch 0, Loss: 0.10082721710205078\n",
      "Epoch 0, Loss: 0.08907061070203781\n",
      "Epoch 0, Loss: 0.09991467744112015\n",
      "Epoch 0, Loss: 0.09398072212934494\n",
      "Epoch 0, Loss: 0.09929242730140686\n",
      "Epoch 0, Loss: 0.08912447839975357\n",
      "Epoch 0, Loss: 0.09352900832891464\n",
      "Epoch 0, Loss: 0.07948564738035202\n",
      "Epoch 0, Loss: 0.08488523215055466\n",
      "Epoch 0, Loss: 0.09674487262964249\n",
      "Epoch 0, Loss: 0.08291739225387573\n",
      "Epoch 0, Loss: 0.07266935706138611\n",
      "Epoch 0, Loss: 0.08494361490011215\n",
      "Epoch 0, Loss: 0.07750970125198364\n",
      "Epoch 0, Loss: 0.07467314600944519\n",
      "Epoch 0, Loss: 0.07284826040267944\n",
      "Epoch 0, Loss: 0.07772378623485565\n",
      "Epoch 0, Loss: 0.0847337543964386\n",
      "Epoch 0, Loss: 0.07572773098945618\n",
      "Epoch 0, Loss: 0.06880739331245422\n",
      "Epoch 0, Loss: 0.07172535359859467\n",
      "Epoch 0, Loss: 0.07896582782268524\n",
      "Epoch 0, Loss: 0.07634985446929932\n",
      "Epoch 0, Loss: 0.07082650065422058\n",
      "Epoch 0, Loss: 0.06887392699718475\n",
      "Epoch 0, Loss: 0.07534929364919662\n",
      "Epoch 0, Loss: 0.06854656338691711\n",
      "Epoch 0, Loss: 0.06954875588417053\n",
      "Epoch 0, Loss: 0.06327013671398163\n",
      "Epoch 0, Loss: 0.0707695484161377\n",
      "Epoch 0, Loss: 0.06687997281551361\n",
      "Epoch 0, Loss: 0.07085860520601273\n",
      "Epoch 0, Loss: 0.06896144896745682\n",
      "Epoch 0, Loss: 0.07192511111497879\n",
      "Epoch 0, Loss: 0.07014099508523941\n",
      "Epoch 0, Loss: 0.070079006254673\n",
      "Epoch 0, Loss: 0.0714833214879036\n",
      "Epoch 0, Loss: 0.058401454240083694\n",
      "Epoch 0, Loss: 0.06544665992259979\n",
      "Epoch 0, Loss: 0.06854059547185898\n",
      "Epoch 0, Loss: 0.07005132734775543\n",
      "Epoch 0, Loss: 0.06508065015077591\n",
      "Epoch 0, Loss: 0.06531384587287903\n",
      "Epoch 0, Loss: 0.06385532021522522\n",
      "Epoch 0, Loss: 0.06615082919597626\n",
      "Epoch 0, Loss: 0.06576314568519592\n",
      "Epoch 0, Loss: 0.06498672068119049\n",
      "Epoch 0, Loss: 0.05907810479402542\n",
      "Epoch 0, Loss: 0.06639142334461212\n",
      "Epoch 0, Loss: 0.06222402676939964\n",
      "Epoch 0, Loss: 0.06884600967168808\n",
      "Epoch 0, Loss: 0.05827707052230835\n",
      "Epoch 0, Loss: 0.05548781156539917\n",
      "Epoch 0, Loss: 0.06209276244044304\n",
      "Epoch 0, Loss: 0.06392203271389008\n",
      "Epoch 0, Loss: 0.06547194719314575\n",
      "Epoch 0, Loss: 0.06207221373915672\n",
      "Epoch 0, Loss: 0.06240040436387062\n",
      "Epoch 0, Loss: 0.053872957825660706\n",
      "Epoch 0, Loss: 0.05638301372528076\n",
      "Epoch 0, Loss: 0.06309553980827332\n",
      "Epoch 0, Loss: 0.05536718666553497\n",
      "Epoch 0, Loss: 0.058489881455898285\n",
      "Epoch 0, Loss: 0.0614086352288723\n",
      "Epoch 0, Loss: 0.06255170702934265\n",
      "Epoch 0, Loss: 0.05905689299106598\n",
      "Epoch 0, Loss: 0.067186638712883\n",
      "Epoch 0, Loss: 0.06124915927648544\n",
      "Epoch 0, Loss: 0.058042675256729126\n",
      "Epoch 0, Loss: 0.05917754024267197\n",
      "Epoch 0, Loss: 0.051491495221853256\n",
      "Epoch 0, Loss: 0.061667680740356445\n",
      "Epoch 0, Loss: 0.05741081386804581\n",
      "Epoch 0, Loss: 0.05577043816447258\n",
      "Epoch 0, Loss: 0.05488244444131851\n",
      "Epoch 0, Loss: 0.06030254438519478\n",
      "Epoch 0, Loss: 0.05932493880391121\n",
      "Epoch 0, Loss: 0.055676721036434174\n",
      "Epoch 0, Loss: 0.05284646898508072\n",
      "Epoch 0, Loss: 0.05263879895210266\n",
      "Epoch 0, Loss: 0.05462172254920006\n",
      "Epoch 0, Loss: 0.0604870468378067\n",
      "Epoch 0, Loss: 0.05885130167007446\n",
      "Epoch 0, Loss: 0.059288665652275085\n",
      "Epoch 0, Loss: 0.05177883431315422\n",
      "Epoch 0, Loss: 0.058592867106199265\n",
      "Epoch 0, Loss: 0.04844698682427406\n",
      "Epoch 0, Loss: 0.05069998279213905\n",
      "Epoch 0, Loss: 0.04772002622485161\n",
      "Epoch 0, Loss: 0.05148492380976677\n",
      "Epoch 0, Loss: 0.058108266443014145\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# 加載數據\n",
    "raw_data = pd.read_csv('train_2022.csv')\n",
    "\n",
    "# 轉換 LABEL 到文本形式\n",
    "raw_data['LABEL'] = raw_data['LABEL'].map({0: \"negative\", 1: \"positive\"})\n",
    "\n",
    "# 創建 T5 格式的輸入\n",
    "raw_data['T5_INPUT'] = \"classify sentiment: \" + raw_data['TEXT']\n",
    "raw_data['T5_OUTPUT'] = raw_data['LABEL']\n",
    "\n",
    "# 切分數據為訓練集和驗證集\n",
    "train_data, val_data = train_test_split(raw_data, test_size=0.1, random_state=42)  # 以10%的數據作為驗證集\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_ids = self.tokenizer.encode(item['T5_INPUT'], \n",
    "                                          max_length=self.max_length, \n",
    "                                          truncation=True, \n",
    "                                          padding='max_length', \n",
    "                                          return_tensors='pt').squeeze()\n",
    "        labels = self.tokenizer.encode(item['T5_OUTPUT'], \n",
    "                                       max_length=self.max_length, \n",
    "                                       truncation=True, \n",
    "                                       padding='max_length', \n",
    "                                       return_tensors='pt').squeeze()\n",
    "        return input_ids, labels\n",
    "\n",
    "# 根據切分後的數據集創建 DataLoader\n",
    "train_dataset = SentimentDataset(train_data, tokenizer)\n",
    "val_dataset = SentimentDataset(val_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# 設定設備\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# 訓練循環\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 訓練模型\n",
    "model.train()\n",
    "for epoch in range(3):  # 使用3個epoch\n",
    "    for input_ids, labels in train_loader:\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "\n",
    "        # 前向傳播\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 反向傳播和優化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in val_loader:\n",
    "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "            outputs = model.generate(input_ids=input_ids)\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            \n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return classification_report(true_labels, predicted_labels, target_names=['negative', 'positive'], output_dict=True)\n",
    "\n",
    "# 在驗證集上評估模型\n",
    "report = evaluate(model, val_loader)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3be2a-5546-4ab3-8516-7dc9afa9298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 加載測試數據\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "\n",
    "# 創建 T5 格式的輸入\n",
    "test_data['T5_INPUT'] = \"classify sentiment: \" + test_data['TEXT']\n",
    "\n",
    "# 定義一個 Dataset 用於測試數據\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_ids = self.tokenizer.encode(item['T5_INPUT'], \n",
    "                                          max_length=self.max_length, \n",
    "                                          truncation=True, \n",
    "                                          padding='max_length', \n",
    "                                          return_tensors='pt').squeeze()\n",
    "        return input_ids\n",
    "\n",
    "# 創建 DataLoader\n",
    "test_dataset = PredictionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab190828-74e3-40db-bf6b-9aed0d82473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids in test_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            outputs = model.generate(input_ids=input_ids)\n",
    "            decoded_preds = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            predictions.extend(decoded_preds)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 生成預測結果\n",
    "predictions = generate_predictions(model, test_loader)\n",
    "\n",
    "# 打印一些預測結果看看\n",
    "for i, text in enumerate(test_data['TEXT'].head(5)):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Sentiment: {predictions[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350aa580-3c40-46b2-a4ec-c86ddc02c890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdc909-0f60-40f5-9257-5e88a98c4436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3073819-7e88-40d0-8c1b-a9890e4a2e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147409d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2f1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fa2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babbab50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51960b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43d5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
