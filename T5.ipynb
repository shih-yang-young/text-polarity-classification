{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56a2fd8c-7f40-4030-9f90-57c99236195f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>classify: director dirk shafer and co-writer g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>classify: a charming , quirky and leisurely pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>classify: the price was good ,  and came quick...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>classify: i was looking forward to this game f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>classify: arguably the year 's silliest and mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>classify: an imaginative comedy\\/thriller .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>classify: a savvy exploration of paranoia and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>classify: on the other hand for power grating ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>classify: like dickens with his passages , mcg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>classify: those who would follow haneke on his...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id                                               TEXT  LABEL\n",
       "0          0  classify: director dirk shafer and co-writer g...      0\n",
       "1          1  classify: a charming , quirky and leisurely pa...      1\n",
       "2          2  classify: the price was good ,  and came quick...      1\n",
       "3          3  classify: i was looking forward to this game f...      0\n",
       "4          4  classify: arguably the year 's silliest and mo...      0\n",
       "...      ...                                                ...    ...\n",
       "1995    1995        classify: an imaginative comedy\\/thriller .      1\n",
       "1996    1996  classify: a savvy exploration of paranoia and ...      1\n",
       "1997    1997  classify: on the other hand for power grating ...      1\n",
       "1998    1998  classify: like dickens with his passages , mcg...      1\n",
       "1999    1999  classify: those who would follow haneke on his...      1\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data = pd.read_csv('train_2022.csv', index_col=False).reset_index(drop=True)\n",
    "add_classify_prefix(raw_data,'TEXT')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f99116d-b2f5-4a6c-97f3-22621d434dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good to know if you can t find these elsewhere .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>love it !  the grill plates come out and pop i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i m convinced this was a poorly executed refur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i would never have complained about that if it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the photo shows the same whole ,  large candie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>10995</td>\n",
       "      <td>i didn t quite get it the first time .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>10996</td>\n",
       "      <td>i ve tried installing with and without the oem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>10997</td>\n",
       "      <td>i was parked at a truck stop in the cincinnati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>10998</td>\n",
       "      <td>i recently bought this case after seeing some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>10999</td>\n",
       "      <td>the keyboard types only % of the time and the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_id                                               TEXT\n",
       "0           0   good to know if you can t find these elsewhere .\n",
       "1           1  love it !  the grill plates come out and pop i...\n",
       "2           2  i m convinced this was a poorly executed refur...\n",
       "3           3  i would never have complained about that if it...\n",
       "4           4  the photo shows the same whole ,  large candie...\n",
       "...       ...                                                ...\n",
       "10995   10995             i didn t quite get it the first time .\n",
       "10996   10996  i ve tried installing with and without the oem...\n",
       "10997   10997  i was parked at a truck stop in the cincinnati...\n",
       "10998   10998  i recently bought this case after seeing some ...\n",
       "10999   10999  the keyboard types only % of the time and the ...\n",
       "\n",
       "[11000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b47b493a-b2c8-4366-93fe-585dbc495657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classify_prefix(dataframe, text_column):\n",
    "    dataframe[text_column] = \"classify: \" + dataframe[text_column].astype(str)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eafacf3c-92e4-4e78-af22-c1cdfabfc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_lowercase(df):\n",
    "    new_df = df.copy();\n",
    "    new_df['TEXT'] = new_df['TEXT'].str.lower()\n",
    "    return new_df\n",
    "import contractions\n",
    "def data_contraction(df):\n",
    "    new_df = df.copy();\n",
    "    new_df['TEXT'] = new_df['TEXT'].apply(lambda x: contractions.fix(x))\n",
    "    return new_df\n",
    "import re\n",
    "def data_remove_tags_punctuations_numbers(df):\n",
    "    new_df = df.copy();\n",
    "    def remove_tags_punctuations_numbers(sentense):\n",
    "        sentense = re.sub(r'<[^>]+>', '', sentense)\n",
    "        sentense = re.sub(r'[^\\w\\s]', '', sentense)\n",
    "        sentense = re.sub(r'\\d+', '', sentense)\n",
    "        return sentense;\n",
    "    new_df['TEXT'] = new_df['TEXT'].apply(lambda x: remove_tags_punctuations_numbers(x))\n",
    "    return new_df\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def data_remove_stopword(df):\n",
    "    new_df = df.copy();\n",
    "    def remove_stopword(sentence):\n",
    "        sentence_arr = sentence.split()\n",
    "        filtered_sentence = [word for word in sentence_arr if word.lower() not in stop_words]\n",
    "        filtered_sentence = ' '.join(filtered_sentence)\n",
    "        return filtered_sentence\n",
    "    new_df['TEXT'] = new_df['TEXT'].apply(lambda x: remove_stopword(x))\n",
    "    return new_df\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def data_lemmatize_text(df):\n",
    "    new_df = df.copy();\n",
    "    def lemmatize_text(text):\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    new_df['TEXT'] = new_df['TEXT'].apply(lambda x: lemmatize_text(x))\n",
    "    return new_df\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "def data_stem(df):\n",
    "    new_df = df.copy();\n",
    "    new_df['TEXT'] = new_df['TEXT'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "    return new_df\n",
    "def data_preprocess(df):\n",
    "    df = data_lowercase(df)\n",
    "    df = data_contraction(df)\n",
    "    df = data_remove_tags_punctuations_numbers(df)\n",
    "    df = data_remove_stopword(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "006f890d-7e3b-4041-a8f3-e4332d9157ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "D:\\Anaconda3\\envs\\dev\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1 Training:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg Eval Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_eval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# 訓練模型\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m train_model(model, tokenizer, train_df, eval_df)\n",
      "Cell \u001b[1;32mIn[32], line 66\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, tokenizer, train_df, eval_df, epochs, batch_size)\u001b[0m\n\u001b[0;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     65\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     67\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     68\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch, labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\dev\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\dev\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\dev\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\dev\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\dev\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\dev\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:260\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key][item] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key. Only three types of key are available: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['TEXT']\n",
    "        label = self.data.iloc[idx]['LABEL']\n",
    "        inputs = self.tokenizer(\n",
    "            \"classify: \" + text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs['labels'] = torch.tensor(label)\n",
    "        return inputs\n",
    "        \n",
    "# 讀取資料\n",
    "raw_data = pd.read_csv('train_2022.csv', index_col=False)\n",
    "\n",
    "# 在 TEXT 欄位加上 classify: 前綴\n",
    "def add_classify_prefix(data, column_name):\n",
    "    data[column_name] = \"classify: \" + data[column_name].astype(str)\n",
    "\n",
    "add_classify_prefix(raw_data, 'TEXT')\n",
    "\n",
    "# 載入 T5 模型和 tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 切分訓練集和驗證集\n",
    "train_df, eval_df = train_test_split(raw_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 定義訓練函式\n",
    "def train_model(model, tokenizer, train_df, eval_df, epochs=3, batch_size=8):\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    train_dataset = tokenizer(train_df['TEXT'].tolist(), truncation=True, padding=True)\n",
    "    eval_dataset = tokenizer(eval_df['TEXT'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch, labels=batch[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Avg Train Loss: {avg_train_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_loader, desc=f\"Epoch {epoch+1} Evaluation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch, labels=batch[\"input_ids\"])\n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "            avg_eval_loss = total_eval_loss / len(eval_loader)\n",
    "            print(f\"Avg Eval Loss: {avg_eval_loss}\")\n",
    "\n",
    "# 訓練模型\n",
    "train_model(model, tokenizer, train_df, eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac6145-7f1c-4367-9dd2-b3dfa9426330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 讀取測試資料\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "\n",
    "# 初始化 T5 模型和 tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('./results')  # 指定訓練完成的模型目錄路徑\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# 資料處理\n",
    "test_inputs = test_data['TEXT'].apply(lambda x: f\"classify: {x}\")  # 加入前綴 \"classify: \"\n",
    "test_encodings = tokenizer(test_inputs.tolist(), truncation=True, padding=True)\n",
    "\n",
    "# 使用模型預測\n",
    "input_ids = test_encodings['input_ids']\n",
    "attention_mask = test_encodings['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "pred_labels = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "# 將預測結果加入測試資料集中\n",
    "test_data['PREDICTED_LABEL'] = pred_labels.tolist()\n",
    "\n",
    "# 儲存包含預測結果的測試資料集\n",
    "test_data.to_csv('test_predicted.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
