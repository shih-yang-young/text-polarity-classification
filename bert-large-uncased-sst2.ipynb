{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa2fbf8f-c44e-4f7a-89f2-12867023cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_2022 = pd.read_csv('train_2022.csv')\n",
    "data_augmentation_chatGPT = pd.read_csv('data_augmentation_chatGPT.csv')\n",
    "data_augmentation_random_2_words = pd.read_csv('data_augmentation_random_2_words.csv')\n",
    "data_augmentation_random_3_words = pd.read_csv('data_augmentation_random_3_words.csv')\n",
    "translated_en_data = pd.read_csv('translated_en_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0596842e-f927-47cf-b188-321e4a56b2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>director dirk shafer and co-writer greg hinton...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a charming , quirky and leisurely paced scotti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the price was good ,  and came quickly though ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i was looking forward to this game for a coupl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>arguably the year 's silliest and most incoher...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1995</td>\n",
       "      <td>An imaginative comedy/thriller.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1996</td>\n",
       "      <td>A savvy exploration of paranoia and insecurity...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1997</td>\n",
       "      <td>On the other hand, with the power grating, you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1998</td>\n",
       "      <td>As in Dickens's novel, McGrath creates many mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1999</td>\n",
       "      <td>Those who follow Haneke on his quest for horro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id                                               TEXT  LABEL\n",
       "0          0  director dirk shafer and co-writer greg hinton...      0\n",
       "1          1  a charming , quirky and leisurely paced scotti...      1\n",
       "2          2  the price was good ,  and came quickly though ...      1\n",
       "3          3  i was looking forward to this game for a coupl...      0\n",
       "4          4  arguably the year 's silliest and most incoher...      0\n",
       "...      ...                                                ...    ...\n",
       "9995    1995                    An imaginative comedy/thriller.      1\n",
       "9996    1996  A savvy exploration of paranoia and insecurity...      1\n",
       "9997    1997  On the other hand, with the power grating, you...      1\n",
       "9998    1998  As in Dickens's novel, McGrath creates many mo...      1\n",
       "9999    1999  Those who follow Haneke on his quest for horro...      1\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.concat([train_2022,data_augmentation_chatGPT,data_augmentation_random_2_words,data_augmentation_random_3_words,translated_en_data], ignore_index=True)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13389a26-bd7e-4c9f-8179-6daaaa899f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.038996696472168\n",
      "Epoch 2, Loss: 0.415835440158844\n",
      "Epoch 3, Loss: 0.19637218117713928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.50      0.50      0.50         4\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.58      0.58      0.58        10\n",
      "weighted avg       0.60      0.60      0.60        10\n",
      "\n",
      "CPU times: total: 3min 28s\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 確認是否有可用的 CUDA 設備，並設定使用的設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "\n",
    "# 載入預訓練的 tokenizer 和模型，這裡指定了 assemblyai 提供的 BERT large 模型\n",
    "tokenizer = AutoTokenizer.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "model.to(device)\n",
    "\n",
    "# 轉換文本為 token IDs\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors='pt', padding=True, truncation=True)['input_ids'][0]\n",
    "\n",
    "merged_data['input_ids'] = merged_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# 轉換成可以輸入模型的格式，並移動到 CUDA 設備上\n",
    "inputs = pad_sequence(merged_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "labels = torch.tensor(merged_data['LABEL'].tolist()).to(device)\n",
    "\n",
    "# 將資料拆分為訓練集和測試集\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建 PyTorch DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 定義 optimizer 和損失函數\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 訓練模型\n",
    "model.train()\n",
    "for epoch in range(5):  # 進行三個 epoch 的訓練\n",
    "    for batch in train_loader:\n",
    "        b_input_ids, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "# 評估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_inputs)\n",
    "    predicted_labels = torch.argmax(test_outputs.logits, dim=1)\n",
    "\n",
    "# 產生分類報告\n",
    "report = classification_report(test_labels.cpu(), predicted_labels.cpu())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aff062e1-b0cd-469c-a44f-9b2e5e6fe4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "def export_csv(df,name):\n",
    "  now = datetime.datetime.now().astimezone(pytz.timezone('Asia/Taipei'))\n",
    "  formatted_time = now.strftime('%Y%m%d')\n",
    "  df.to_csv('result/'+ formatted_time + '_' + name + \".csv\", index=False,encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc51cb81-30d0-4f9c-8e61-9f270692546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id                                               TEXT  LABEL\n",
      "0           0   good to know if you can t find these elsewhere .      1\n",
      "1           1  love it !  the grill plates come out and pop i...      1\n",
      "2           2  i m convinced this was a poorly executed refur...      0\n",
      "3           3  i would never have complained about that if it...      1\n",
      "4           4  the photo shows the same whole ,  large candie...      1\n",
      "...       ...                                                ...    ...\n",
      "10995   10995             i didn t quite get it the first time .      0\n",
      "10996   10996  i ve tried installing with and without the oem...      0\n",
      "10997   10997  i was parked at a truck stop in the cincinnati...      0\n",
      "10998   10998  i recently bought this case after seeing some ...      1\n",
      "10999   10999  the keyboard types only % of the time and the ...      0\n",
      "\n",
      "[11000 rows x 3 columns]\n",
      "CPU times: total: 29.9 s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 設定 device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 載入預測資料集\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "\n",
    "# 使用 tokenizer 將文本轉換為 token IDs\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "test_data['input_ids'] = test_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# 轉換成可以輸入模型的格式\n",
    "test_inputs = pad_sequence(test_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "\n",
    "# 創建 PyTorch DataLoader\n",
    "test_dataset = TensorDataset(test_inputs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# 使用模型進行預測\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = model(inputs[0])  # 確保 inputs[0] 已在 GPU 上\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "# 將預測結果添加到測試數據集中\n",
    "test_data['LABEL'] = predictions\n",
    "# 保存預測結果到 CSV 文件\n",
    "# 需要你自己定義 export_csv 函數，或使用 pandas 的 to_csv 方法\n",
    "export_csv(test_data[['row_id', 'LABEL']], 'bert_large_uncased_sst2')\n",
    "# 打印預測結果\n",
    "print(test_data[['row_id','TEXT', 'LABEL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0187c-12cb-46cc-b24a-095713b3f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e3496-4f54-4e65-af66-a8b4e247f050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa80c8-184c-4d67-a960-7bed4f5f1313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0db879e-5ea2-48f2-8a3f-434d634184f5",
   "metadata": {},
   "source": [
    "# 多加一層MLP 用那層tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979ccf64-0eed-4da4-844f-d5cf8630e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.29902413487434387\n",
      "Epoch 2, Loss: 0.3729929029941559\n",
      "Epoch 3, Loss: 0.9795500636100769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       416\n",
      "           1       0.79      0.79      0.79       384\n",
      "\n",
      "    accuracy                           0.80       800\n",
      "   macro avg       0.80      0.80      0.80       800\n",
      "weighted avg       0.80      0.80      0.80       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # 設定設備\n",
    "# device = \"cpu\"\n",
    "\n",
    "# # 載入預訓練的 tokenizer 和模型\n",
    "# tokenizer = AutoTokenizer.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "\n",
    "# # 凍結預訓練層的權重\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 新增一個自定義層，只有這層是可訓練的\n",
    "# num_labels = 2  # 假設是一個二分類問題\n",
    "# custom_classifier = nn.Sequential(\n",
    "#     nn.Linear(model.classifier.in_features, num_labels)\n",
    "# )\n",
    "# custom_classifier.to(device)\n",
    "\n",
    "# # 將自定義層加到模型上\n",
    "# model.classifier = custom_classifier\n",
    "\n",
    "# # 轉換文本為 token IDs\n",
    "# def tokenize_text(text):\n",
    "#     return tokenizer(text, return_tensors='pt', padding=True, truncation=True)['input_ids'][0]\n",
    "\n",
    "# merged_data['input_ids'] = merged_data['TEXT'].apply(tokenize_text)\n",
    "# inputs = pad_sequence(merged_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "# labels = torch.tensor(merged_data['LABEL'].tolist()).to(device)\n",
    "\n",
    "# # 將資料拆分為訓練集和測試集\n",
    "# train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 創建 DataLoader\n",
    "# train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# # 定義 optimizer 只針對新層\n",
    "# optimizer = AdamW(model.classifier.parameters(), lr=2e-5)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # 訓練模型\n",
    "# model.train()\n",
    "# for epoch in range(3):\n",
    "#     for b_input_ids, b_labels in train_loader:\n",
    "#         model.zero_grad()\n",
    "#         outputs = model(b_input_ids, labels=b_labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# # 評估模型\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_outputs = model(test_inputs)\n",
    "#     predicted_labels = torch.argmax(test_outputs.logits, dim=1)\n",
    "\n",
    "# # 產生分類報告\n",
    "# report = classification_report(test_labels.cpu(), predicted_labels.cpu())\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c50d4-b810-415b-9ec6-b5cd434e9871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81c18293-a5af-4e61-9983-5515be0c49d9",
   "metadata": {},
   "source": [
    "# 直接使用 model 做預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4343ce57-a761-49fe-a083-89127d62eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79        49\n",
      "           1       0.80      0.78      0.79        51\n",
      "\n",
      "    accuracy                           0.79       100\n",
      "   macro avg       0.79      0.79      0.79       100\n",
      "weighted avg       0.79      0.79      0.79       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "# from torch.nn.functional import softmax\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # 設置設備\n",
    "# device = \"cpu\"\n",
    "\n",
    "# # 載入預訓練的 tokenizer 和模型\n",
    "# tokenizer = AutoTokenizer.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # 轉換文本為模型輸入格式\n",
    "# def prepare_data(texts):\n",
    "#     encoding = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "#     return encoding['input_ids'], encoding['attention_mask']\n",
    "\n",
    "# # 使用模型進行批次預測\n",
    "# def predict(texts):\n",
    "#     input_ids, attention_mask = prepare_data(texts)\n",
    "#     input_ids = input_ids.to(device)\n",
    "#     attention_mask = attention_mask.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#     logits = outputs.logits\n",
    "#     probabilities = softmax(logits, dim=1)\n",
    "#     return probabilities\n",
    "\n",
    "# # 預測情感\n",
    "# probabilities = predict(merged_data['TEXT'].tolist())\n",
    "# predicted_labels = torch.argmax(probabilities, dim=1).numpy()\n",
    "\n",
    "# # 實際標籤\n",
    "# real_labels = merged_data['LABEL'].tolist()\n",
    "\n",
    "# # 計算分類報告\n",
    "# report = classification_report(real_labels, predicted_labels)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b4af5-56b6-468f-9c4a-e5f3a5061c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757042a-d720-43d9-ba4d-e1d8971764f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
