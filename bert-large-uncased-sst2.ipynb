{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2fbf8f-c44e-4f7a-89f2-12867023cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_2022 = pd.read_csv('train_2022.csv')\n",
    "data_augmentation_chatGPT = pd.read_csv('data_augmentation_chatGPT.csv')\n",
    "data_augmentation_random_2_words = pd.read_csv('data_augmentation_random_2_words.csv')\n",
    "data_augmentation_random_3_words = pd.read_csv('data_augmentation_random_3_words.csv')\n",
    "translated_en_data = pd.read_csv('translated_en_data.csv')\n",
    "amazon_short_text_data = pd.read_csv('amazon_short_text_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009999ba-3c26-47e2-b828-8680ac0adc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296977</th>\n",
       "      <td>0</td>\n",
       "      <td>Hole on the bottom? I like the glasses, but I ...</td>\n",
       "      <td>296977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362068</th>\n",
       "      <td>0</td>\n",
       "      <td>Great Authors Low Rate Stories These are four ...</td>\n",
       "      <td>362068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52409</th>\n",
       "      <td>0</td>\n",
       "      <td>TERRIBLE This book is as boring as watching pa...</td>\n",
       "      <td>52409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117723</th>\n",
       "      <td>0</td>\n",
       "      <td>Just did not get there Don't even waste your m...</td>\n",
       "      <td>117723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417931</th>\n",
       "      <td>0</td>\n",
       "      <td>Be Very Careful In three days of using this pr...</td>\n",
       "      <td>417931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96922</th>\n",
       "      <td>0</td>\n",
       "      <td>Cliche The jokes were recycled and the movie w...</td>\n",
       "      <td>96922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394533</th>\n",
       "      <td>0</td>\n",
       "      <td>What's On It? Nowhere does it tell how many vi...</td>\n",
       "      <td>394533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262900</th>\n",
       "      <td>0</td>\n",
       "      <td>Garbage. I suppose you get what you pay for. I...</td>\n",
       "      <td>262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438808</th>\n",
       "      <td>0</td>\n",
       "      <td>Musical This is a rock musical set in 1960s Am...</td>\n",
       "      <td>438808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394291</th>\n",
       "      <td>0</td>\n",
       "      <td>BORING!!!!!!!!!!! This is the most boring book...</td>\n",
       "      <td>394291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LABEL                                               TEXT  row_id\n",
       "296977      0  Hole on the bottom? I like the glasses, but I ...  296977\n",
       "362068      0  Great Authors Low Rate Stories These are four ...  362068\n",
       "52409       0  TERRIBLE This book is as boring as watching pa...   52409\n",
       "117723      0  Just did not get there Don't even waste your m...  117723\n",
       "417931      0  Be Very Careful In three days of using this pr...  417931\n",
       "...       ...                                                ...     ...\n",
       "96922       0  Cliche The jokes were recycled and the movie w...   96922\n",
       "394533      0  What's On It? Nowhere does it tell how many vi...  394533\n",
       "262900      0  Garbage. I suppose you get what you pay for. I...  262900\n",
       "438808      0  Musical This is a rock musical set in 1960s Am...  438808\n",
       "394291      0  BORING!!!!!!!!!!! This is the most boring book...  394291\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random = 4000\n",
    "amazon_short_text_zero_data = amazon_short_text_data[amazon_short_text_data['LABEL'] == 0]\n",
    "amazon_short_text_zero_data = amazon_short_text_zero_data.sample(n=random, random_state=42)\n",
    "amazon_short_text_zero_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e0e7f0-35b0-45cd-94d4-6bda6fe43bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97764</th>\n",
       "      <td>1</td>\n",
       "      <td>grat value and product great price, was cheape...</td>\n",
       "      <td>97764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278643</th>\n",
       "      <td>1</td>\n",
       "      <td>their best album This group's third album is m...</td>\n",
       "      <td>278643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368891</th>\n",
       "      <td>1</td>\n",
       "      <td>Paying too much? Well I think 50 dollars for t...</td>\n",
       "      <td>368891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182252</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing Product I LOVE This gate. It was so ha...</td>\n",
       "      <td>182252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181656</th>\n",
       "      <td>1</td>\n",
       "      <td>The Little Mermaid The Little Mermaid is a cla...</td>\n",
       "      <td>181656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247830</th>\n",
       "      <td>1</td>\n",
       "      <td>one of the best shows on tv i have never laugh...</td>\n",
       "      <td>247830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462387</th>\n",
       "      <td>1</td>\n",
       "      <td>fun movie the kids and the adults all liked th...</td>\n",
       "      <td>462387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478759</th>\n",
       "      <td>1</td>\n",
       "      <td>It was the perfect addition to my homemade \"Ch...</td>\n",
       "      <td>478759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446645</th>\n",
       "      <td>1</td>\n",
       "      <td>Awesome This is an awesome product. It will wi...</td>\n",
       "      <td>446645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41209</th>\n",
       "      <td>1</td>\n",
       "      <td>Hilarious! This book made me laugh at least fi...</td>\n",
       "      <td>41209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LABEL                                               TEXT  row_id\n",
       "97764       1  grat value and product great price, was cheape...   97764\n",
       "278643      1  their best album This group's third album is m...  278643\n",
       "368891      1  Paying too much? Well I think 50 dollars for t...  368891\n",
       "182252      1  Amazing Product I LOVE This gate. It was so ha...  182252\n",
       "181656      1  The Little Mermaid The Little Mermaid is a cla...  181656\n",
       "...       ...                                                ...     ...\n",
       "247830      1  one of the best shows on tv i have never laugh...  247830\n",
       "462387      1  fun movie the kids and the adults all liked th...  462387\n",
       "478759      1  It was the perfect addition to my homemade \"Ch...  478759\n",
       "446645      1  Awesome This is an awesome product. It will wi...  446645\n",
       "41209       1  Hilarious! This book made me laugh at least fi...   41209\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_short_text_one_data = amazon_short_text_data[amazon_short_text_data['LABEL'] == 1]\n",
    "amazon_short_text_one_data = amazon_short_text_one_data.sample(n=random, random_state=42)\n",
    "amazon_short_text_one_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3cc4277-323d-460a-999e-6eb78d4f56b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>director dirk shafer and co-writer greg hinton...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a charming , quirky and leisurely paced scotti...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the price was good ,  and came quickly though ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was looking forward to this game for a coupl...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arguably the year 's silliest and most incoher...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>one of the best shows on tv i have never laugh...</td>\n",
       "      <td>1</td>\n",
       "      <td>9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>fun movie the kids and the adults all liked th...</td>\n",
       "      <td>1</td>\n",
       "      <td>9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>It was the perfect addition to my homemade \"Ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>9997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Awesome This is an awesome product. It will wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Hilarious! This book made me laugh at least fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   TEXT  LABEL  row_id\n",
       "0     director dirk shafer and co-writer greg hinton...      0       0\n",
       "1     a charming , quirky and leisurely paced scotti...      1       1\n",
       "2     the price was good ,  and came quickly though ...      1       2\n",
       "3     i was looking forward to this game for a coupl...      0       3\n",
       "4     arguably the year 's silliest and most incoher...      0       4\n",
       "...                                                 ...    ...     ...\n",
       "9995  one of the best shows on tv i have never laugh...      1    9995\n",
       "9996  fun movie the kids and the adults all liked th...      1    9996\n",
       "9997  It was the perfect addition to my homemade \"Ch...      1    9997\n",
       "9998  Awesome This is an awesome product. It will wi...      1    9998\n",
       "9999  Hilarious! This book made me laugh at least fi...      1    9999\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.concat([train_2022, amazon_short_text_zero_data, amazon_short_text_one_data], ignore_index=True)\n",
    "merged_data = merged_data.reset_index(drop=True)\n",
    "merged_data = merged_data.drop(columns=['row_id'])\n",
    "merged_data['row_id'] = merged_data.index\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13389a26-bd7e-4c9f-8179-6daaaa899f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|          | 0/2667 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Epoch 1:   0%|          | 1/2667 [00:15<11:18:52, 15.28s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 120.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:49\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:578\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    576\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[0;32m    577\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m--> 578\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    580\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 確認是否有可用的 CUDA 設備，並設定使用的設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "\n",
    "# 載入預訓練的 tokenizer 和模型，這裡指定了 assemblyai 提供的 BERT large 模型\n",
    "tokenizer = AutoTokenizer.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "model.to(device)\n",
    "\n",
    "# 轉換文本為 token IDs\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors='pt', padding=True, truncation=True)['input_ids'][0]\n",
    "\n",
    "merged_data['input_ids'] = merged_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# 轉換成可以輸入模型的格式，並移動到 CUDA 設備上\n",
    "inputs = pad_sequence(merged_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "labels = torch.tensor(merged_data['LABEL'].tolist()).to(device)\n",
    "\n",
    "# 將資料拆分為訓練集和測試集\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建 PyTorch DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# 定義 optimizer 和損失函數\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 訓練模型\n",
    "model.train()\n",
    "for epoch in range(5):  # 進行個 epoch 的訓練\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}'):  # 使用tqdm包裹train_loader\n",
    "        b_input_ids, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "# 評估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_inputs)\n",
    "    predicted_labels = torch.argmax(test_outputs.logits, dim=1)\n",
    "\n",
    "# 產生分類報告\n",
    "report = classification_report(test_labels.cpu(), predicted_labels.cpu())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5017a1e6-fb77-492e-8a60-b7d34a85c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/bert-large-uncased-sst2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff062e1-b0cd-469c-a44f-9b2e5e6fe4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "def export_csv(df,name):\n",
    "  now = datetime.datetime.now().astimezone(pytz.timezone('Asia/Taipei'))\n",
    "  formatted_time = now.strftime('%Y%m%d')\n",
    "  df.to_csv('result/'+ formatted_time + '_' + name + \".csv\", index=False,encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc51cb81-30d0-4f9c-8e61-9f270692546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id                                               TEXT  LABEL\n",
      "0           0   good to know if you can t find these elsewhere .      1\n",
      "1           1  love it !  the grill plates come out and pop i...      1\n",
      "2           2  i m convinced this was a poorly executed refur...      0\n",
      "3           3  i would never have complained about that if it...      1\n",
      "4           4  the photo shows the same whole ,  large candie...      0\n",
      "...       ...                                                ...    ...\n",
      "10995   10995             i didn t quite get it the first time .      0\n",
      "10996   10996  i ve tried installing with and without the oem...      1\n",
      "10997   10997  i was parked at a truck stop in the cincinnati...      0\n",
      "10998   10998  i recently bought this case after seeing some ...      1\n",
      "10999   10999  the keyboard types only % of the time and the ...      0\n",
      "\n",
      "[11000 rows x 3 columns]\n",
      "CPU times: total: 58.2 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 設定 device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 載入預測資料集\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "\n",
    "# 使用 tokenizer 將文本轉換為 token IDs\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "test_data['input_ids'] = test_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# 轉換成可以輸入模型的格式\n",
    "test_inputs = pad_sequence(test_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "\n",
    "# 創建 PyTorch DataLoader\n",
    "test_dataset = TensorDataset(test_inputs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# 使用模型進行預測\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = model(inputs[0])  # 確保 inputs[0] 已在 GPU 上\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "# 將預測結果添加到測試數據集中\n",
    "test_data['LABEL'] = predictions\n",
    "# 保存預測結果到 CSV 文件\n",
    "# 需要你自己定義 export_csv 函數，或使用 pandas 的 to_csv 方法\n",
    "export_csv(test_data[['row_id', 'LABEL']], 'bert_large_uncased_sst2_Amazon_10000')\n",
    "# 打印預測結果\n",
    "print(test_data[['row_id','TEXT', 'LABEL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0187c-12cb-46cc-b24a-095713b3f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e3496-4f54-4e65-af66-a8b4e247f050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa80c8-184c-4d67-a960-7bed4f5f1313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0db879e-5ea2-48f2-8a3f-434d634184f5",
   "metadata": {},
   "source": [
    "# 多加一層MLP 用那層tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979ccf64-0eed-4da4-844f-d5cf8630e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # 設定設備\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # 載入預訓練的 tokenizer 和模型\n",
    "# tokenizer = AutoTokenizer.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "\n",
    "# # 凍結預訓練層的權重\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 新增一個自定義層，只有這層是可訓練的\n",
    "# num_labels = 2  # 假設是一個二分類問題\n",
    "# custom_classifier = nn.Sequential(\n",
    "#     nn.Linear(model.classifier.in_features, num_labels)\n",
    "# )\n",
    "\n",
    "# # 將自定義層加到模型上\n",
    "# model.classifier = custom_classifier\n",
    "# model.to(device)  # 確保整個模型都在同一設備上\n",
    "\n",
    "# # 轉換文本為 token IDs\n",
    "# def tokenize_text(text):\n",
    "#     return tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)['input_ids'][0]\n",
    "\n",
    "# merged_data['input_ids'] = merged_data['TEXT'].apply(tokenize_text)\n",
    "# inputs = pad_sequence(merged_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "# labels = torch.tensor(merged_data['LABEL'].tolist()).to(device)\n",
    "\n",
    "# # 將資料拆分為訓練集和測試集\n",
    "# train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 創建 DataLoader\n",
    "# train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # 定義 optimizer 只針對新層\n",
    "# optimizer = AdamW(model.classifier.parameters(), lr=2e-5)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # 訓練模型\n",
    "# model.train()\n",
    "# for epoch in range(5):\n",
    "#     for b_input_ids, b_labels in train_loader:\n",
    "#         model.zero_grad()\n",
    "#         outputs = model(b_input_ids, labels=b_labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# # 評估模型\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_outputs = model(test_inputs)\n",
    "#     predicted_labels = torch.argmax(test_outputs.logits, dim=1)\n",
    "\n",
    "# # 產生分類報告\n",
    "# report = classification_report(test_labels.cpu(), predicted_labels.cpu())\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c50d4-b810-415b-9ec6-b5cd434e9871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81c18293-a5af-4e61-9983-5515be0c49d9",
   "metadata": {},
   "source": [
    "# 直接使用 model 做預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4343ce57-a761-49fe-a083-89127d62eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "# from torch.nn.functional import softmax\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # 設置設備\n",
    "# device = \"cpu\"\n",
    "\n",
    "# # 載入預訓練的 tokenizer 和模型\n",
    "# tokenizer = AutoTokenizer.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('assemblyai/bert-large-uncased-sst2')\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # 轉換文本為模型輸入格式\n",
    "# def prepare_data(texts):\n",
    "#     encoding = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "#     return encoding['input_ids'], encoding['attention_mask']\n",
    "\n",
    "# # 使用模型進行批次預測\n",
    "# def predict(texts):\n",
    "#     input_ids, attention_mask = prepare_data(texts)\n",
    "#     input_ids = input_ids.to(device)\n",
    "#     attention_mask = attention_mask.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#     logits = outputs.logits\n",
    "#     probabilities = softmax(logits, dim=1)\n",
    "#     return probabilities\n",
    "\n",
    "# # 預測情感\n",
    "# probabilities = predict(merged_data['TEXT'].tolist())\n",
    "# predicted_labels = torch.argmax(probabilities, dim=1).numpy()\n",
    "\n",
    "# # 實際標籤\n",
    "# real_labels = merged_data['LABEL'].tolist()\n",
    "\n",
    "# # 計算分類報告\n",
    "# report = classification_report(real_labels, predicted_labels)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b4af5-56b6-468f-9c4a-e5f3a5061c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757042a-d720-43d9-ba4d-e1d8971764f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
