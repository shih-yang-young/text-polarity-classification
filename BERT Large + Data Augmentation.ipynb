{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb25652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_2022 = pd.read_csv('train_2022.csv')\n",
    "data_augmentation_chatGPT = pd.read_csv('data_augmentation_chatGPT.csv')\n",
    "data_augmentation_random_2_words = pd.read_csv('data_augmentation_random_2_words.csv')\n",
    "data_augmentation_random_3_words = pd.read_csv('data_augmentation_random_3_words.csv')\n",
    "translated_en_data = pd.read_csv('translated_en_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa788f8-fe33-4fea-b756-07c4725f1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_random_data = pd.concat([data_augmentation_random_2_words, data_augmentation_random_3_words], ignore_index=True)\n",
    "merged_random_data = merged_random_data.sample(n=2000, random_state=42)\n",
    "merged_random_data = merged_random_data.reset_index(drop=True)\n",
    "# merged_random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff4d95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>director dirk shafer and co-writer greg hinton...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a charming , quirky and leisurely paced scotti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the price was good ,  and came quickly though ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i was looking forward to this game for a coupl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>arguably the year 's silliest and most incoher...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>1995</td>\n",
       "      <td>A creative comedy/thriller.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>1996</td>\n",
       "      <td>Explores paranoia and insecurity in America's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>1997</td>\n",
       "      <td>Good for power grating.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>1998</td>\n",
       "      <td>McGrath's variation on the novel crafts moving...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>1999</td>\n",
       "      <td>Haneke's creepy explorations are rewarded with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id                                               TEXT  LABEL\n",
       "0          0  director dirk shafer and co-writer greg hinton...      0\n",
       "1          1  a charming , quirky and leisurely paced scotti...      1\n",
       "2          2  the price was good ,  and came quickly though ...      1\n",
       "3          3  i was looking forward to this game for a coupl...      0\n",
       "4          4  arguably the year 's silliest and most incoher...      0\n",
       "...      ...                                                ...    ...\n",
       "7995    1995                        A creative comedy/thriller.      1\n",
       "7996    1996  Explores paranoia and insecurity in America's ...      1\n",
       "7997    1997                            Good for power grating.      1\n",
       "7998    1998  McGrath's variation on the novel crafts moving...      1\n",
       "7999    1999  Haneke's creepy explorations are rewarded with...      1\n",
       "\n",
       "[8000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.concat([train_2022, merged_random_data,translated_en_data, data_augmentation_chatGPT], ignore_index=True)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d17a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # 載入預訓練的 BERT tokenizer 和模型\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)  # 2 表示二分類，正向和負向情感\n",
    "\n",
    "# # 轉換成 DataFrame\n",
    "# train_data = merged_data.copy()\n",
    "\n",
    "# # 使用 tokenizer 將文本轉換為 token IDs\n",
    "# def tokenize_text(text):\n",
    "#     return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "# train_data['input_ids'] = train_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# # 轉換成可以輸入模型的格式\n",
    "# inputs = pad_sequence(train_data['input_ids'].tolist(), batch_first=True)\n",
    "\n",
    "# # 將資料拆分為訓練集和測試集\n",
    "# train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, train_data['LABEL'].tolist(), test_size=0.2)\n",
    "\n",
    "# # 創建 PyTorch DataLoader\n",
    "# train_dataset = TensorDataset(train_inputs, torch.tensor(train_labels))\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # 定義 optimizer 和損失函數\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # 訓練模型\n",
    "# model.train()\n",
    "# for epoch in range(5):\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = loss_fn(outputs.logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # 評估模型\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(test_inputs)\n",
    "#     predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "# # 計算分類報告\n",
    "# report = classification_report(test_labels, predicted_labels)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb4c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 檢查CUDA是否可用\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d71a0-48d3-404c-8156-d729dd43515e",
   "metadata": {},
   "source": [
    "# cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82229514-a504-48e0-8bdb-d7015c4f65c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.66       797\n",
      "           1       0.00      0.00      0.00       803\n",
      "\n",
      "    accuracy                           0.50      1600\n",
      "   macro avg       0.25      0.50      0.33      1600\n",
      "weighted avg       0.25      0.50      0.33      1600\n",
      "\n",
      "CPU times: total: 11min 11s\n",
      "Wall time: 20min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Program Files\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Program Files\\anaconda3\\envs\\Pytorch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 確認是否有可用的 CUDA 設備，並設定使用的設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 載入預訓練的 BERT tokenizer 和模型，並將它們移動到 CUDA 設備上\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# 轉換成 DataFrame\n",
    "train_data = merged_data.copy()\n",
    "\n",
    "# 使用 tokenizer 將文本轉換為 token IDs\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "train_data['input_ids'] = train_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# 轉換成可以輸入模型的格式，並移動到 CUDA 設備上\n",
    "inputs = pad_sequence(train_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "\n",
    "# 將資料拆分為訓練集和測試集\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, train_data['LABEL'].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建 PyTorch DataLoader，並設定為使用 CUDA\n",
    "train_dataset = TensorDataset(train_inputs, torch.tensor(train_labels).to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# 定義 optimizer 和損失函數\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 訓練模型\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 將資料移動到 CUDA 設備上\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 評估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_inputs = test_inputs.to(device)  # 將測試資料移動到 CUDA 設備上\n",
    "    outputs = model(test_inputs)\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "# 計算分類報告\n",
    "report = classification_report(test_labels, predicted_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d1e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "def export_csv(df,name):\n",
    "  now = datetime.datetime.now().astimezone(pytz.timezone('Asia/Taipei'))\n",
    "  formatted_time = now.strftime('%Y%m%d')\n",
    "  df.to_csv('result/'+ formatted_time + '_' + name + \".csv\", index=False,encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff4e6520-bb32-4a78-bb49-80c9da24b962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id                                               TEXT  LABEL\n",
      "0           0   good to know if you can t find these elsewhere .      0\n",
      "1           1  love it !  the grill plates come out and pop i...      0\n",
      "2           2  i m convinced this was a poorly executed refur...      0\n",
      "3           3  i would never have complained about that if it...      0\n",
      "4           4  the photo shows the same whole ,  large candie...      0\n",
      "...       ...                                                ...    ...\n",
      "10995   10995             i didn t quite get it the first time .      0\n",
      "10996   10996  i ve tried installing with and without the oem...      0\n",
      "10997   10997  i was parked at a truck stop in the cincinnati...      0\n",
      "10998   10998  i recently bought this case after seeing some ...      0\n",
      "10999   10999  the keyboard types only % of the time and the ...      0\n",
      "\n",
      "[11000 rows x 3 columns]\n",
      "CPU times: total: 57.6 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 設定 device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 載入預測資料集\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "\n",
    "# 使用 tokenizer 將文本轉換為 token IDs\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "test_data['input_ids'] = test_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# 轉換成可以輸入模型的格式\n",
    "test_inputs = pad_sequence(test_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "\n",
    "# 創建 PyTorch DataLoader\n",
    "test_dataset = TensorDataset(test_inputs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# 使用模型進行預測\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = model(inputs[0])  # 確保 inputs[0] 已在 GPU 上\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "# 將預測結果添加到測試數據集中\n",
    "test_data['LABEL'] = predictions\n",
    "# 保存預測結果到 CSV 文件\n",
    "# 需要你自己定義 export_csv 函數，或使用 pandas 的 to_csv 方法\n",
    "export_csv(test_data[['row_id', 'LABEL']], 'BERT_Large_DataAugmentation')\n",
    "# 打印預測結果\n",
    "print(test_data[['row_id','TEXT', 'LABEL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c4f2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 載入預測資料集\n",
    "# test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "\n",
    "# # 使用 tokenizer 將文本轉換為 token IDs\n",
    "# def tokenize_text(text):\n",
    "#     return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "# test_data['input_ids'] = test_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# # 轉換成可以輸入模型的格式\n",
    "# test_inputs = pad_sequence(test_data['input_ids'].tolist(), batch_first=True)\n",
    "\n",
    "# # 創建 PyTorch DataLoader\n",
    "# test_dataset = TensorDataset(test_inputs)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# # 使用模型進行預測\n",
    "# model.eval()\n",
    "# predictions = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs in test_loader:\n",
    "#         outputs = model(inputs[0])  # inputs[0] 是 token IDs\n",
    "#         predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "#         predictions.extend(predicted_labels)\n",
    "\n",
    "# # 將預測結果添加到測試數據集中\n",
    "# test_data['LABEL'] = predictions\n",
    "# # 保存預測結果到 CSV 文件\n",
    "# export_csv(test_data.drop(columns=['TEXT','input_ids']),'BERT_Large_DataAugmentation')\n",
    "\n",
    "# # 打印預測結果\n",
    "# print(test_data[['TEXT', 'LABEL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8023e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d5dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ac00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26fc92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
