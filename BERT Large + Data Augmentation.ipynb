{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb25652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_2022 = pd.read_csv('train_2022.csv')\n",
    "data_augmentation_chatGPT = pd.read_csv('data_augmentation_chatGPT.csv')\n",
    "data_augmentation_random_2_words = pd.read_csv('data_augmentation_random_2_words.csv')\n",
    "data_augmentation_random_3_words = pd.read_csv('data_augmentation_random_3_words.csv')\n",
    "translated_en_data = pd.read_csv('translated_en_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa788f8-fe33-4fea-b756-07c4725f1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_random_data = pd.concat([data_augmentation_random_2_words, data_augmentation_random_3_words], ignore_index=True)\n",
    "merged_random_data = merged_random_data.sample(n=2000, random_state=42)\n",
    "merged_random_data = merged_random_data.reset_index(drop=True)\n",
    "# merged_random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff4d95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>1802</td>\n",
       "      <td>a movie far more cynical and lazy than anythin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>828</td>\n",
       "      <td>in num_num thq came out with this wrestling ga...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>1709</td>\n",
       "      <td>i would not recommend this item for dogs who c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>1267</td>\n",
       "      <td>what you get with empire is a movie you 've se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>1536</td>\n",
       "      <td>reinforces the often forgotten fact of the wor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>1575</td>\n",
       "      <td>the charging light illuminates ,  but does not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>309</td>\n",
       "      <td>speedloader is very hard to use especially wit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>1595</td>\n",
       "      <td>then contact sharp and go on from there .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>it keeps the air clean and nice fresh smelling .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>1097</td>\n",
       "      <td>some movies are like a tasty hors-d'oeuvre ; t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id                                               TEXT  LABEL\n",
       "1802    1802  a movie far more cynical and lazy than anythin...      0\n",
       "828      828  in num_num thq came out with this wrestling ga...      0\n",
       "1709    1709  i would not recommend this item for dogs who c...      0\n",
       "1267    1267  what you get with empire is a movie you 've se...      0\n",
       "1536    1536  reinforces the often forgotten fact of the wor...      1\n",
       "...      ...                                                ...    ...\n",
       "1575    1575  the charging light illuminates ,  but does not...      0\n",
       "309      309  speedloader is very hard to use especially wit...      0\n",
       "1595    1595          then contact sharp and go on from there .      0\n",
       "39        39   it keeps the air clean and nice fresh smelling .      1\n",
       "1097    1097  some movies are like a tasty hors-d'oeuvre ; t...      1\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.concat([train_2022], ignore_index=True)\n",
    "merged_data = merged_data.sample(n=100)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d17a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # 載入預訓練的 BERT tokenizer 和模型\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)  # 2 表示二分類，正向和負向情感\n",
    "\n",
    "# # 轉換成 DataFrame\n",
    "# train_data = merged_data.copy()\n",
    "\n",
    "# # 使用 tokenizer 將文本轉換為 token IDs\n",
    "# def tokenize_text(text):\n",
    "#     return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "# train_data['input_ids'] = train_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# # 轉換成可以輸入模型的格式\n",
    "# inputs = pad_sequence(train_data['input_ids'].tolist(), batch_first=True)\n",
    "\n",
    "# # 將資料拆分為訓練集和測試集\n",
    "# train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, train_data['LABEL'].tolist(), test_size=0.2)\n",
    "\n",
    "# # 創建 PyTorch DataLoader\n",
    "# train_dataset = TensorDataset(train_inputs, torch.tensor(train_labels))\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # 定義 optimizer 和損失函數\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # 訓練模型\n",
    "# model.train()\n",
    "# for epoch in range(5):\n",
    "#     for inputs, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = loss_fn(outputs.logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # 評估模型\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(test_inputs)\n",
    "#     predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "# # 計算分類報告\n",
    "# report = classification_report(test_labels, predicted_labels)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb4c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 檢查CUDA是否可用\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d71a0-48d3-404c-8156-d729dd43515e",
   "metadata": {},
   "source": [
    "# cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82229514-a504-48e0-8bdb-d7015c4f65c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.20      0.29        10\n",
      "           1       0.50      0.80      0.62        10\n",
      "\n",
      "    accuracy                           0.50        20\n",
      "   macro avg       0.50      0.50      0.45        20\n",
      "weighted avg       0.50      0.50      0.45        20\n",
      "\n",
      "CPU times: total: 10min 5s\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 確認是否有可用的 CUDA 設備，並設定使用的設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "# 載入預訓練的 BERT tokenizer 和模型，並將它們移動到 CUDA 設備上\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# 轉換成 DataFrame\n",
    "train_data = merged_data.copy()\n",
    "\n",
    "# 使用 tokenizer 將文本轉換為 token IDs\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "train_data['input_ids'] = train_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# 轉換成可以輸入模型的格式，並移動到 CUDA 設備上\n",
    "inputs = pad_sequence(train_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "\n",
    "# 將資料拆分為訓練集和測試集\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, train_data['LABEL'].tolist(), test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建 PyTorch DataLoader，並設定為使用 CUDA\n",
    "train_dataset = TensorDataset(train_inputs, torch.tensor(train_labels).to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# 定義 optimizer 和損失函數\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 訓練模型\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 將資料移動到 CUDA 設備上\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "# 評估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_inputs = test_inputs.to(device)  # 將測試資料移動到 CUDA 設備上\n",
    "    outputs = model(test_inputs)\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "# 計算分類報告\n",
    "report = classification_report(test_labels, predicted_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d1e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "def export_csv(df,name):\n",
    "  now = datetime.datetime.now().astimezone(pytz.timezone('Asia/Taipei'))\n",
    "  formatted_time = now.strftime('%Y%m%d')\n",
    "  df.to_csv('result/'+ formatted_time + '_' + name + \".csv\", index=False,encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff4e6520-bb32-4a78-bb49-80c9da24b962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id                                               TEXT  LABEL\n",
      "0           0   good to know if you can t find these elsewhere .      0\n",
      "1           1  love it !  the grill plates come out and pop i...      0\n",
      "2           2  i m convinced this was a poorly executed refur...      0\n",
      "3           3  i would never have complained about that if it...      0\n",
      "4           4  the photo shows the same whole ,  large candie...      0\n",
      "...       ...                                                ...    ...\n",
      "10995   10995             i didn t quite get it the first time .      0\n",
      "10996   10996  i ve tried installing with and without the oem...      0\n",
      "10997   10997  i was parked at a truck stop in the cincinnati...      0\n",
      "10998   10998  i recently bought this case after seeing some ...      0\n",
      "10999   10999  the keyboard types only % of the time and the ...      0\n",
      "\n",
      "[11000 rows x 3 columns]\n",
      "CPU times: total: 57.6 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 設定 device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 載入預測資料集\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "\n",
    "# 使用 tokenizer 將文本轉換為 token IDs\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "test_data['input_ids'] = test_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# 轉換成可以輸入模型的格式\n",
    "test_inputs = pad_sequence(test_data['input_ids'].tolist(), batch_first=True).to(device)\n",
    "\n",
    "# 創建 PyTorch DataLoader\n",
    "test_dataset = TensorDataset(test_inputs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# 使用模型進行預測\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = model(inputs[0])  # 確保 inputs[0] 已在 GPU 上\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "        predictions.extend(predicted_labels)\n",
    "\n",
    "# 將預測結果添加到測試數據集中\n",
    "test_data['LABEL'] = predictions\n",
    "# 保存預測結果到 CSV 文件\n",
    "# 需要你自己定義 export_csv 函數，或使用 pandas 的 to_csv 方法\n",
    "export_csv(test_data[['row_id', 'LABEL']], 'BERT_Large_DataAugmentation')\n",
    "# 打印預測結果\n",
    "print(test_data[['row_id','TEXT', 'LABEL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c4f2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 載入預測資料集\n",
    "# test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "\n",
    "# # 使用 tokenizer 將文本轉換為 token IDs\n",
    "# def tokenize_text(text):\n",
    "#     return tokenizer(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "# test_data['input_ids'] = test_data['TEXT'].apply(tokenize_text)\n",
    "\n",
    "# # 轉換成可以輸入模型的格式\n",
    "# test_inputs = pad_sequence(test_data['input_ids'].tolist(), batch_first=True)\n",
    "\n",
    "# # 創建 PyTorch DataLoader\n",
    "# test_dataset = TensorDataset(test_inputs)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# # 使用模型進行預測\n",
    "# model.eval()\n",
    "# predictions = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs in test_loader:\n",
    "#         outputs = model(inputs[0])  # inputs[0] 是 token IDs\n",
    "#         predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "#         predictions.extend(predicted_labels)\n",
    "\n",
    "# # 將預測結果添加到測試數據集中\n",
    "# test_data['LABEL'] = predictions\n",
    "# # 保存預測結果到 CSV 文件\n",
    "# export_csv(test_data.drop(columns=['TEXT','input_ids']),'BERT_Large_DataAugmentation')\n",
    "\n",
    "# # 打印預測結果\n",
    "# print(test_data[['TEXT', 'LABEL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8023e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350d5dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ac00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26fc92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
