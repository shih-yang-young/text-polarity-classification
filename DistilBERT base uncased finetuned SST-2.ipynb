{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4546cbdd-f0ef-4e4c-bc9f-640d08b8b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_2022 = pd.read_csv('train_2022.csv')\n",
    "data_augmentation_chatGPT = pd.read_csv('data_augmentation_chatGPT.csv')\n",
    "data_augmentation_random_2_words = pd.read_csv('data_augmentation_random_2_words.csv')\n",
    "data_augmentation_random_3_words = pd.read_csv('data_augmentation_random_3_words.csv')\n",
    "translated_en_data = pd.read_csv('translated_en_data.csv')\n",
    "amazon_short_text_data = pd.read_csv('amazon_short_text_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07d58eb8-5980-4a70-83eb-69a2fb90a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "random = 9000\n",
    "amazon_short_text_zero_data = amazon_short_text_data[amazon_short_text_data['LABEL'] == 0]\n",
    "amazon_short_text_zero_data = amazon_short_text_zero_data.sample(n=random, random_state=42)\n",
    "amazon_short_text_one_data = amazon_short_text_data[amazon_short_text_data['LABEL'] == 1]\n",
    "amazon_short_text_one_data = amazon_short_text_one_data.sample(n=random, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5054acb8-2fd6-4d7e-8631-94ae01ad42fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>director dirk shafer and co-writer greg hinton...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a charming , quirky and leisurely paced scotti...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the price was good ,  and came quickly though ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was looking forward to this game for a coupl...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arguably the year 's silliest and most incoher...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>Great piano music Great music if you like clas...</td>\n",
       "      <td>1</td>\n",
       "      <td>19995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>Excellent Album Another fantastic album by Arm...</td>\n",
       "      <td>1</td>\n",
       "      <td>19996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>iPod Dock and Speaker Station PRODUCT WAS JUST...</td>\n",
       "      <td>1</td>\n",
       "      <td>19997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>Yea Man - is the Best Put the Top down and cra...</td>\n",
       "      <td>1</td>\n",
       "      <td>19998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>A Masterpiece Once again, Bruce puts his finge...</td>\n",
       "      <td>1</td>\n",
       "      <td>19999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    TEXT  LABEL  row_id\n",
       "0      director dirk shafer and co-writer greg hinton...      0       0\n",
       "1      a charming , quirky and leisurely paced scotti...      1       1\n",
       "2      the price was good ,  and came quickly though ...      1       2\n",
       "3      i was looking forward to this game for a coupl...      0       3\n",
       "4      arguably the year 's silliest and most incoher...      0       4\n",
       "...                                                  ...    ...     ...\n",
       "19995  Great piano music Great music if you like clas...      1   19995\n",
       "19996  Excellent Album Another fantastic album by Arm...      1   19996\n",
       "19997  iPod Dock and Speaker Station PRODUCT WAS JUST...      1   19997\n",
       "19998  Yea Man - is the Best Put the Top down and cra...      1   19998\n",
       "19999  A Masterpiece Once again, Bruce puts his finge...      1   19999\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.concat([train_2022, amazon_short_text_zero_data, amazon_short_text_one_data], ignore_index=True)\n",
    "merged_data = merged_data.reset_index(drop=True)\n",
    "merged_data = merged_data.drop(columns=['row_id'])\n",
    "merged_data['row_id'] = merged_data.index\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0939f95-727f-4b3c-9fea-1860eb60f9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 3200/3200 [06:17<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.009683920070528984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3200/3200 [06:05<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.032490458339452744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3200/3200 [06:00<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0017600416904315352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3200/3200 [05:48<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.010685762390494347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 3200/3200 [06:03<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0009067291393876076\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.46 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:68\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:994\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    992\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 994\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[0;32m    995\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    996\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    997\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    998\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    999\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1000\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1001\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1002\u001b[0m )\n\u001b[0;32m   1003\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:806\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    804\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 806\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[0;32m    809\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:145\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[1;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[0;32m    142\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m input_embeds \u001b[38;5;241m+\u001b[39m position_embeddings  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    146\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlayer_norm(\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\functional.py:2573\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2571\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2572\u001b[0m     )\n\u001b[1;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlayer_norm(\u001b[38;5;28minput\u001b[39m, normalized_shape, weight, bias, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.46 GiB. GPU "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 確認CUDA是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "# 載入預訓練的 DistilBERT tokenizer 和模型，並將它們移動到CUDA設備上\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# 讀取數據\n",
    "train_data = merged_data.copy()\n",
    "\n",
    "# 使用 tokenizer 將文本轉換為 token IDs 和注意力遮罩\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length=128, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_data['encoding'] = train_data['TEXT'].apply(tokenize_text)\n",
    "train_data['input_ids'] = train_data['encoding'].apply(lambda x: x['input_ids'].squeeze(0))\n",
    "train_data['attention_mask'] = train_data['encoding'].apply(lambda x: x['attention_mask'].squeeze(0))\n",
    "\n",
    "# 將資料拆分為訓練集和測試集\n",
    "train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
    "    torch.stack(train_data['input_ids'].tolist()),\n",
    "    torch.stack(train_data['attention_mask'].tolist()),\n",
    "    train_data['LABEL'].tolist(), \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 創建 PyTorch DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# 定義 optimizer 和損失函數\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 訓練模型\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}'):  # 使用tqdm包裹train_loader\n",
    "        inputs, masks, labels = tuple(t.to(device) for t in batch)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# 評估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_inputs, test_masks = test_inputs.to(device), test_masks.to(device)\n",
    "    outputs = model(input_ids=test_inputs, attention_mask=test_masks)\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "# 計算分類報告\n",
    "report = classification_report(test_labels, predicted_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbde7a06-023b-4286-bf8c-45a228dda4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/DistilBERT_20000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dfb3c27-0194-4ad9-afdb-c08adb542a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 250/250 [00:16<00:00, 15.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      2045\n",
      "           1       0.94      0.93      0.93      1955\n",
      "\n",
      "    accuracy                           0.93      4000\n",
      "   macro avg       0.93      0.93      0.93      4000\n",
      "weighted avg       0.93      0.93      0.93      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "model.load_state_dict(torch.load('models/DistilBERT_20000.pth'))\n",
    "# 定義一個函數來處理分批預測\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(predicted_labels)\n",
    "    return predictions\n",
    "\n",
    "# 創建數據加載器，您可以調整batch_size以避免OOM\n",
    "batch_size = 16  # 您可以根據GPU的容量進行調整\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "# 計算分類報告\n",
    "predicted_labels = evaluate_model(model, test_dataloader)\n",
    "report = classification_report(test_labels, predicted_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdb1c419-984b-43a6-aed3-9516f9ddc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "def export_csv(df,name):\n",
    "  now = datetime.datetime.now().astimezone(pytz.timezone('Asia/Taipei'))\n",
    "  formatted_time = now.strftime('%Y%m%d')\n",
    "  df.to_csv('result/'+ formatted_time + '_' + name + \".csv\", index=False,encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2536c3d5-7d5f-48e2-b6e3-866031b942be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id                                               TEXT  LABEL\n",
      "0           0   good to know if you can t find these elsewhere .      0\n",
      "1           1  love it !  the grill plates come out and pop i...      1\n",
      "2           2  i m convinced this was a poorly executed refur...      0\n",
      "3           3  i would never have complained about that if it...      1\n",
      "4           4  the photo shows the same whole ,  large candie...      1\n",
      "...       ...                                                ...    ...\n",
      "10995   10995             i didn t quite get it the first time .      0\n",
      "10996   10996  i ve tried installing with and without the oem...      1\n",
      "10997   10997  i was parked at a truck stop in the cincinnati...      0\n",
      "10998   10998  i recently bought this case after seeing some ...      1\n",
      "10999   10999  the keyboard types only % of the time and the ...      0\n",
      "\n",
      "[11000 rows x 3 columns]\n",
      "CPU times: total: 26 s\n",
      "Wall time: 51.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 預處理測試數據\n",
    "test_data = pd.read_csv('test_no_answer_2022.csv')\n",
    "result = test_data.copy()\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length=128, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "test_data['encoding'] = test_data['TEXT'].apply(tokenize_text)\n",
    "test_data['input_ids'] = test_data['encoding'].apply(lambda x: x['input_ids'].squeeze(0))\n",
    "test_data['attention_mask'] = test_data['encoding'].apply(lambda x: x['attention_mask'].squeeze(0))\n",
    "\n",
    "# 將處理好的數據轉換為 DataLoader\n",
    "test_inputs = torch.stack(test_data['input_ids'].tolist())\n",
    "test_masks = torch.stack(test_data['attention_mask'].tolist())\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)  # 可以調整batch size\n",
    "\n",
    "# 使用模型進行預測\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, masks = tuple(t.to(device) for t in batch)\n",
    "        outputs = model(input_ids=inputs, attention_mask=masks)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "# 將預測結果附加到 DataFrame 並保存或打印結果\n",
    "result['LABEL'] = predictions\n",
    "print(result[['row_id', 'TEXT', 'LABEL']])\n",
    "\n",
    "# 可選：將預測結果保存到 CSV 文件\n",
    "export_csv(result.drop(columns=['TEXT']),'Distill_BERT_FineTune_SST2_20000_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d899a4-d019-4321-abe2-d92a236a1d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>encoding</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good to know if you can t find these elsewhere .</td>\n",
       "      <td>{'input_ids': tensor([[ 101, 2204, 2000, 2113,...</td>\n",
       "      <td>tensor([ 101, 2204, 2000, 2113, 2065, 2017, 20...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>love it !  the grill plates come out and pop i...</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2293,  2009,   ...</td>\n",
       "      <td>tensor([  101,  2293,  2009,   999,  1996, 186...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i m convinced this was a poorly executed refur...</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1045,  1049,  6...</td>\n",
       "      <td>tensor([  101,  1045,  1049,  6427,  2023,  20...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i would never have complained about that if it...</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1045,  2052,  2...</td>\n",
       "      <td>tensor([  101,  1045,  2052,  2196,  2031, 108...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the photo shows the same whole ,  large candie...</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1996,  6302,  3...</td>\n",
       "      <td>tensor([  101,  1996,  6302,  3065,  1996,  21...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>10995</td>\n",
       "      <td>i didn t quite get it the first time .</td>\n",
       "      <td>{'input_ids': tensor([[ 101, 1045, 2134, 1056,...</td>\n",
       "      <td>tensor([ 101, 1045, 2134, 1056, 3243, 2131, 20...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>10996</td>\n",
       "      <td>i ve tried installing with and without the oem...</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1045,  2310,  2...</td>\n",
       "      <td>tensor([  101,  1045,  2310,  2699, 23658,  20...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>10997</td>\n",
       "      <td>i was parked at a truck stop in the cincinnati...</td>\n",
       "      <td>{'input_ids': tensor([[ 101, 1045, 2001, 9083,...</td>\n",
       "      <td>tensor([ 101, 1045, 2001, 9083, 2012, 1037, 47...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>10998</td>\n",
       "      <td>i recently bought this case after seeing some ...</td>\n",
       "      <td>{'input_ids': tensor([[ 101, 1045, 3728, 4149,...</td>\n",
       "      <td>tensor([ 101, 1045, 3728, 4149, 2023, 2553, 20...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>10999</td>\n",
       "      <td>the keyboard types only % of the time and the ...</td>\n",
       "      <td>{'input_ids': tensor([[ 101, 1996, 9019, 4127,...</td>\n",
       "      <td>tensor([ 101, 1996, 9019, 4127, 2069, 1003, 19...</td>\n",
       "      <td>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_id                                               TEXT  \\\n",
       "0           0   good to know if you can t find these elsewhere .   \n",
       "1           1  love it !  the grill plates come out and pop i...   \n",
       "2           2  i m convinced this was a poorly executed refur...   \n",
       "3           3  i would never have complained about that if it...   \n",
       "4           4  the photo shows the same whole ,  large candie...   \n",
       "...       ...                                                ...   \n",
       "10995   10995             i didn t quite get it the first time .   \n",
       "10996   10996  i ve tried installing with and without the oem...   \n",
       "10997   10997  i was parked at a truck stop in the cincinnati...   \n",
       "10998   10998  i recently bought this case after seeing some ...   \n",
       "10999   10999  the keyboard types only % of the time and the ...   \n",
       "\n",
       "                                                encoding  \\\n",
       "0      {'input_ids': tensor([[ 101, 2204, 2000, 2113,...   \n",
       "1      {'input_ids': tensor([[  101,  2293,  2009,   ...   \n",
       "2      {'input_ids': tensor([[  101,  1045,  1049,  6...   \n",
       "3      {'input_ids': tensor([[  101,  1045,  2052,  2...   \n",
       "4      {'input_ids': tensor([[  101,  1996,  6302,  3...   \n",
       "...                                                  ...   \n",
       "10995  {'input_ids': tensor([[ 101, 1045, 2134, 1056,...   \n",
       "10996  {'input_ids': tensor([[  101,  1045,  2310,  2...   \n",
       "10997  {'input_ids': tensor([[ 101, 1045, 2001, 9083,...   \n",
       "10998  {'input_ids': tensor([[ 101, 1045, 3728, 4149,...   \n",
       "10999  {'input_ids': tensor([[ 101, 1996, 9019, 4127,...   \n",
       "\n",
       "                                               input_ids  \\\n",
       "0      tensor([ 101, 2204, 2000, 2113, 2065, 2017, 20...   \n",
       "1      tensor([  101,  2293,  2009,   999,  1996, 186...   \n",
       "2      tensor([  101,  1045,  1049,  6427,  2023,  20...   \n",
       "3      tensor([  101,  1045,  2052,  2196,  2031, 108...   \n",
       "4      tensor([  101,  1996,  6302,  3065,  1996,  21...   \n",
       "...                                                  ...   \n",
       "10995  tensor([ 101, 1045, 2134, 1056, 3243, 2131, 20...   \n",
       "10996  tensor([  101,  1045,  2310,  2699, 23658,  20...   \n",
       "10997  tensor([ 101, 1045, 2001, 9083, 2012, 1037, 47...   \n",
       "10998  tensor([ 101, 1045, 3728, 4149, 2023, 2553, 20...   \n",
       "10999  tensor([ 101, 1996, 9019, 4127, 2069, 1003, 19...   \n",
       "\n",
       "                                          attention_mask  predicted_labels  \n",
       "0      tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 1  \n",
       "1      tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 1  \n",
       "2      tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 0  \n",
       "3      tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 1  \n",
       "4      tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 1  \n",
       "...                                                  ...               ...  \n",
       "10995  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,...                 0  \n",
       "10996  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 0  \n",
       "10997  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 0  \n",
       "10998  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 1  \n",
       "10999  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...                 0  \n",
       "\n",
       "[11000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = pd.read_csv('test_predictions.csv')\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd5bf9-621b-47df-ac42-f5675f4a7a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa98c94-6f59-4d6b-80b7-281de57c0234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2ae92-0a11-4e02-8409-920d97b7cc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b3860-ec48-445b-9ae9-4aa67a829e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664ebf8-90b6-48cf-99a5-98584302afff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4899c296-eb8d-4e6c-afc0-7d8558603bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
